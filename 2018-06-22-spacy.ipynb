{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick post on spaCy\n",
    "\n",
    "It's been a few days since I've posted, so this is a quick post about what I've been experimenting with: `spaCy`, a natural language processing library.\n",
    "\n",
    "\n",
    "## Why use a natural language processing library like `spaCy`\n",
    "\n",
    "Natural language processing gets complicated fast.\n",
    "For example, it's surprisingly tricky to divide text into sentences and words. A naive approach would be to split on whitespace and periods. It's easy to find a sentence that breaks these rules, such as\n",
    "\n",
    "    I'll drive to Mt. Hood on Friday!\n",
    "\n",
    "An NLP library like `spaCy` can divide `I'll` into two separate tokens `I` and `'ll`.  The library can also tell that not all periods are the end of a sentence (e.g. `Mt.`), and that there is punctuation other than `.` (e.g. `!`). These rules will depend on the language; `spaCy` has an English model that works for my purposes.\n",
    "\n",
    "Aside from sentence boundary detection and tokenization, `spaCy` can tag parts-of-speech of words (`drive` is a `VERB`), say `'ll` is the same as `will`, parse the sentence (the `drive` is `on Friday`), along with other [linguistics features](https://spacy.io/usage/linguistic-features). It also has a nice set-up for adding custom attributes using [pipelines](https://spacy.io/usage/processing-pipelines). \n",
    "\n",
    "\n",
    "An alternative natural language processing library to `spaCy` is [`nltk`](https://www.nltk.org). `nltk` also comes with a lovely free [book](https://www.nltk.org/book/) on natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation spaCy\n",
    "\n",
    "The lovely [documentation](https://spacy.io/usage/) explain how to install the package and a language model. I installed the English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence boundary detection and tokenization\n",
    "\n",
    "I can use `nlp` to parse the text and get a `Doc`. This takes a bit of time, but then further processing is fast. The length of the `Doc` (`len(doc)`) gives the number of words (`Tokens`). To get the number of sentences, I can count the sentences (`Span`) from `doc.sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "words\t\t47\n",
      "sentences\t3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''\\\n",
    "I wondered if I could write a program to automatically catch clumsy style mistakes I often make.\n",
    "I'll try using spaCy!\n",
    "It turns out style-checking is a little complicated, so this post is actually just about spaCy.\n",
    "''')\n",
    "\n",
    "print('''\n",
    "words\\t\\t{num_words}\n",
    "sentences\\t{num_sent}\n",
    "'''.format(\n",
    "    num_words=len(doc),  # grab number of tokens\n",
    "    num_sent=len(list(doc.sents)),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    words\t\t47\n",
    "    sentences\t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "I can also see how `spaCy` tokenizes my example from above:\n",
    "\n",
    "     I'll drive to Mt. Hood on Friday!\n",
    "     \n",
    "Indeed, `spaCy` doesn't split the sentence at `Mt.` and does split `I'll` into the tokens `I` and `'ll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t'll\tdrive\tto\tMt.\tHood\ton\tFriday\t!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I'll drive to Mt. Hood on Friday!\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print('\\t'.join(str(token) for token in sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I\t'll\tdrive\tto\tMt.\tHood\ton\tFriday\t!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "I can get root words by checking out what `token.lemma_` gives (`.lemma` without the underscore is a special ID.)\n",
    "It converts `'ll` into `will` and `Mt.` into `Mount`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t'll\tdrive\tto\tMt.\tHood\ton\tFriday\t!\n",
      "-PRON-\twill\tdrive\tto\tMount\thood\ton\tfriday\t!\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print('\\t'.join(str(token) for token in sentence))\n",
    "    print('\\t'.join(token.lemma_ for token in sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I\t'll\tdrive\tto\tMt.\tHood\ton\tFriday\t!\n",
    "    -PRON-\twill\tdrive\tto\tMount\thood\ton\tfriday\t!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: highlighting words\n",
    "\n",
    "Switching gears for a moment, I can use `IPython.display` to make funner output in Jupyter notebook [like before](https://jessicastringham.net/2018/05/07/reading-jupyter-notebooks-into-Python.html). `highlight_doc` will take a `Doc` and a function that says whether a given token should be highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def _highlight_word(word):\n",
    "    return '<span style=\"color:blue\">**{}**</span>'.format(word)\n",
    "\n",
    "def highlight_doc(doc, should_highlight_func):\n",
    "    '''Display a word.\n",
    "\n",
    "    doc: spacy.Doc that should be highlighted\n",
    "    should_highlight_func: a function that takes in a spacy.Token and returns True\n",
    "      or False depending on if the token should be highlighted\n",
    "    '''\n",
    "    for sentence in doc.sents:\n",
    "        markdown_sentence = []\n",
    "        for token in sentence:\n",
    "            markdown_word = token.text\n",
    "\n",
    "            if should_highlight_func(token):\n",
    "                markdown_word = _highlight_word(markdown_word)\n",
    "\n",
    "            markdown_sentence.append(markdown_word)\n",
    "        display(Markdown(' '.join(markdown_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighting verbs\n",
    "\n",
    "To test the UI, I can highlight verbs by checking the [`token`'s `pos` attribute](https://spacy.io/api/annotation#pos-tagging). (In this case, I can use `.pos` instead of `.pos_` so I can compare with `spacy.symbols.VERB`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I <span style=\"color:blue\">**wondered**</span> if I <span style=\"color:blue\">**could**</span> <span style=\"color:blue\">**write**</span> a program to automatically <span style=\"color:blue\">**catch**</span> clumsy style mistakes I often <span style=\"color:blue\">**make**</span> . \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I <span style=\"color:blue\">**'ll**</span> <span style=\"color:blue\">**try**</span> <span style=\"color:blue\">**using**</span> spaCy ! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "It <span style=\"color:blue\">**turns**</span> out style - checking <span style=\"color:blue\">**is**</span> a little complicated , so this post <span style=\"color:blue\">**is**</span> actually just about spaCy . \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.symbols import VERB\n",
    "\n",
    "doc = nlp('''\\\n",
    "I wondered if I could write a program to automatically catch clumsy style mistakes I often make.\n",
    "I'll try using spaCy!\n",
    "It turns out style-checking is a little complicated, so this post is actually just about spaCy.\n",
    "''')\n",
    "\n",
    "highlight_doc(doc, lambda token: token.pos == VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I <span style=\"color:blue\">**wondered**</span> if I <span style=\"color:blue\">**could**</span> <span style=\"color:blue\">**write**</span> a program to automatically <span style=\"color:blue\">**catch**</span> clumsy style mistakes I often <span style=\"color:blue\">**make**</span> . \n",
    "\n",
    "I <span style=\"color:blue\">**'ll**</span> <span style=\"color:blue\">**try**</span> <span style=\"color:blue\">**using**</span> spaCy ! \n",
    "\n",
    "It <span style=\"color:blue\">**turns**</span> out style - checking <span style=\"color:blue\">**is**</span> a little complicated , so this post <span style=\"color:blue\">**is**</span> actually just about spaCy . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities\n",
    "\n",
    "`spaCy` also extracts a few neat natural language processing. For example, it can highlight [named entities](https://spacy.io/usage/linguistic-features#section-named-entities), which is often hard to do!\n",
    "It says Mt. Hood is a \"Buildings, airports, highways, bridges, etc.\" Neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I 'll drive to <span style=\"color:blue\">**Mt.**</span> <span style=\"color:blue\">**Hood**</span> on <span style=\"color:blue\">**Friday**</span> !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mt. Hood FAC\n",
      "Friday DATE\n"
     ]
    }
   ],
   "source": [
    "# this will be a little hard to read if noun chunks are near each other\n",
    "doc = nlp(\"I'll drive to Mt. Hood on Friday!\")\n",
    "\n",
    "# get a list of token indexes that are in a noun_chunk\n",
    "is_in_named_entity = set(sum((list(range(entity.start, entity.end)) for entity in doc.ents), []))\n",
    "\n",
    "highlight_doc(doc, lambda token: token.i in is_in_named_entity)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I 'll drive to <span style=\"color:blue\">**Mt.**</span> <span style=\"color:blue\">**Hood**</span> on <span style=\"color:blue\">**Friday**</span> !\n",
    "\n",
    "    Mt. Hood FAC\n",
    "    Friday DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etc\n",
    "\n",
    "This was a quick post introducing a few features of `spaCy`. Assembling them into a real project is another challenge!\n",
    "\n",
    "`spaCy` is an interesting project. It's neat to see how NLP and AI can be used in a usable package. \n",
    "The [`spaCy`](https://spacy.io) documentation is lots of fun. One tip is to jump between similarly-named sections, like POS tagging, in \"Usage\", \"Models\", and \"API\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
